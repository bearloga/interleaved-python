{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4da6f38-b496-44b0-aa10-88ec1e6f8676",
   "metadata": {},
   "source": [
    "# 2017 interleaved A/B test of machine learned ranking\n",
    "\n",
    "Prior to 2016, our search engine used term frequency—inverse document frequency ([tf—idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) for ranking documents (e.g. articles and other pages on English Wikipedia). After successful A/B testing, we switched to [BM25 scoring algorithm](https://en.wikipedia.org/wiki/Okapi_BM25) which was used production on almost all languages – except a few space-less languages. After that we focused our efforts on information retrieval using [machine-learned ranking](https://en.wikipedia.org/wiki/Learning_to_rank) (MLR). In MLR, a model is trained to predict a document’s relevance from various document-level and query-level features which represent the document.\n",
    "\n",
    "[MjoLniR](https://gerrit.wikimedia.org/g/search/MjoLniR) – our Python and Spark-based library for handling the backend data processing for Machine Learned Ranking at Wikimedia – uses a click-based [Dynamic Bayesian Network](https://en.wikipedia.org/wiki/Dynamic_Bayesian_network) (Chapelle and Zhang 2009) (implemented via [ClickModels](https://github.com/varepsilon/clickmodels) Python library) to create relevance labels for training data fed into XGBoost.\n",
    "\n",
    "## Data\n",
    "\n",
    "This example uses archived data from the 2017 test of our machine-learned ranking functions. The data was collected using the [SearchSatisfaction instrument](https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/WikimediaEvents/+/c7b48d995dbbbe6767b3d3ef452b5aea68ce7a60/modules/ext.wikimediaEvents/searchSatisfaction.js) ([schema](https://gerrit.wikimedia.org/r/plugins/gitiles/schemas/event/secondary/+/30087c7f6910f7ca917ede504f2c8498edb29216/jsonschema/analytics/legacy/searchsatisfaction/current.yaml)). Sessions with 50 or more searches were excluded from the analysis, due to them potentially being automated/bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ad20c8-d4e8-483f-a165-d206420ed506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interleaved import Experiment\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c07f514-f705-4673-abcd-1ed8ca14c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_test = pd.read_csv('learning_to_rank_2017.csv')\n",
    "ltr_test = ltr_test.groupby(ltr_test.group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a693d2-8a92-45fe-bc0c-6b4fdf494e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = ltr_test.get_group(\"ltr-i-1024\")\n",
    "test_2 = ltr_test.get_group(\"ltr-i-20\")\n",
    "test_3 = ltr_test.get_group(\"ltr-i-20-1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4caf2-22b5-4f5f-8c66-df063d3d78c8",
   "metadata": {},
   "source": [
    "## BM-25 vs MLR-20\n",
    "\n",
    "The \"MLR-20\" model used a rescore window of 20. This means that each shard (of which English Wikipedia has 7) applies the model to the top 20 results. Those 140 results are then collected and sorted to produce the top 20 shown to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e8e411-555a-454f-9b61-482ec5d0f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In this interleaved search experiment, 4854 searches were used to determine whether the\n",
      "results from ranker 'BM-25' or 'MLR-20' were preferred by users (based on their clicks to\n",
      "the results from those rankers interleaved into a single search result set).\n",
      "\n",
      " The preference statistic, as defined by Chapelle et al. (2012), was estimated to be -4.0%\n",
      "with a 95% (bootstrapped) confidence interval of (-6.8%, -1.5%) on [-100%, 100%] scale\n",
      "with -100% indicating total preference for 'MLR-20', 100% indicating total preference for\n",
      "'BM-25', and 0% indicating complete lack of preference between the two -- indicating that\n",
      "the users had preference for ranker 'MLR-20'.\n"
     ]
    }
   ],
   "source": [
    "experiment_1 = Experiment(\n",
    "    queries = test_1['search_id'].to_numpy(),\n",
    "    clicks = test_1['team'].to_numpy()\n",
    ")\n",
    "experiment_1.bootstrap(seed=20)\n",
    "print(experiment_1.summary(rescale=True, ranker_labels=['BM-25', 'MLR-20']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4929d-2b6a-4575-b801-38669295e3ba",
   "metadata": {},
   "source": [
    "## BM-25 vs MLR-1024\n",
    "\n",
    "The \"MLR-1024\" model used a rescore window of 1024. This means that each of the seven shards applies the model to the top 1024 results. Those 7168 results are then collected and sorted to produce the final top 20 (or top 15) shown to the users, since almost no users look at results beyond the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0d6b09-04b6-43c1-8f74-245ad1b86d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In this interleaved search experiment, 4603 searches were used to determine whether the\n",
      "results from ranker 'BM-25' or 'MLR-1024' were preferred by users (based on their clicks\n",
      "to the results from those rankers interleaved into a single search result set).\n",
      "\n",
      " The preference statistic, as defined by Chapelle et al. (2012), was estimated to be -1.7%\n",
      "with a 95% (bootstrapped) confidence interval of (-4.4%, 1.4%) on [-100%, 100%] scale with\n",
      "-100% indicating total preference for 'MLR-1024', 100% indicating total preference for\n",
      "'BM-25', and 0% indicating complete lack of preference between the two -- indicating that\n",
      "the users had no preference for either ranker.\n"
     ]
    }
   ],
   "source": [
    "experiment_2 = Experiment(\n",
    "    queries = test_2['search_id'].to_numpy(),\n",
    "    clicks = test_2['team'].to_numpy()\n",
    ")\n",
    "experiment_2.bootstrap(seed=1024)\n",
    "print(experiment_2.summary(rescale=True, ranker_labels=['BM-25', 'MLR-1024']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e63b28-c43e-49a7-b387-819842234e0f",
   "metadata": {},
   "source": [
    "## MLR-20 vs MLR-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994e887a-3d35-43f0-a261-abbb7663d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In this interleaved search experiment, 4709 searches were used to determine whether the\n",
      "results from ranker 'MLR-20' or 'MLR-1024' were preferred by users (based on their clicks\n",
      "to the results from those rankers interleaved into a single search result set).\n",
      "\n",
      " The preference statistic, as defined by Chapelle et al. (2012), was estimated to be -0.1%\n",
      "with a 95% (bootstrapped) confidence interval of (-3.0%, 2.6%) on [-100%, 100%] scale with\n",
      "-100% indicating total preference for 'MLR-1024', 100% indicating total preference for\n",
      "'MLR-20', and 0% indicating complete lack of preference between the two -- indicating that\n",
      "the users had no preference for either ranker.\n"
     ]
    }
   ],
   "source": [
    "experiment_3 = Experiment(\n",
    "    queries = test_3['search_id'].to_numpy(),\n",
    "    clicks = test_3['team'].to_numpy()\n",
    ")\n",
    "experiment_3.bootstrap(seed=1004)\n",
    "print(experiment_3.summary(rescale=True, ranker_labels=['MLR-20', 'MLR-1024']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad59e59-e8a3-4ad5-a63b-3e60172d2663",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## References\n",
    "\n",
    "Chapelle, O., & Zhang, Y. (2009). *A dynamic bayesian network click model for web search ranking*. New York, New York, USA: ACM.\n",
    "\n",
    "Chapelle, O., Joachims, T., Radlinski, F., & Yue, Y. (2012). Large-scale validation and analysis of interleaved search evaluation. *ACM Trans. Inf. Syst.*, **30**(1), 6:1–6:41. doi:10.1145/2094072.2094078"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
